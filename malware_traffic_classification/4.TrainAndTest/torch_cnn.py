import torch
import torch.nn as nn
from matplotlib import pyplot as plt
from torch.utils.data import DataLoader
import warnings
import torchvision
import myDataSets
from torch.autograd import Variable
from torchviz import make_dot
from torchnet import meter
from visual_loss import Visualizer

EPOCH = 100
BATCH_SIZE = 64
LR = 0.001

#可视化loss
# vis = Visualizer(env='my_wind')
# loss_meter = meter.AverageValueMeter()

# 调用GPU
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 忽略警告信息
warnings.filterwarnings("ignore", category=UserWarning)

torch.manual_seed(1)

train_data = myDataSets.myDataSets(
    root='../3.PreprocessedResults/20class/SessionAllLayers/',
    train=True,
    transform=torchvision.transforms.ToTensor()
)

train_loader = DataLoader(dataset=train_data, shuffle=True, batch_size=BATCH_SIZE)

print(train_data.train_data.size())

test_data = myDataSets.myDataSets(
    root='../3.PreprocessedResults/20class/SessionAllLayers/',  # dataset存储路径
    train=False,  # True表示是train训练集，False表示test测试集
    transform=torchvision.transforms.ToTensor(),  # 将原数据规范化到（0,1）区间
)

test_loader = DataLoader(dataset=test_data, shuffle=True, batch_size=BATCH_SIZE)

print(test_data.train_data.size())

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Sequential(  # (1,28,28)
            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5,
                      stride=1, padding=2),  # (16,28,28)
            # 想要conv2d卷积出来的图片尺寸没有变化, padding=(kernel_size-1)/2
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)  # (16,14,14)
        )
        self.conv2 = nn.Sequential(  # (16,14,14)
            nn.Conv2d(16, 64, 5, 1, 2),  # (64,14,14)
            nn.ReLU(),
            nn.MaxPool2d(2)  # (64,7,7)
        )
        self.fc1 = nn.Sequential(
            nn.Linear(64 * 7 * 7, 1024),
            nn.ReLU(),
            nn.Dropout(0.5)
        )
        self.out = nn.Linear(1024, 20)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = x.view(x.size(0), -1)  # 将（batch，64,7,7）展平为（batch，64*7*7）
        x = self.fc1(x)
        output = self.out(x)
        return output

cnn = CNN().to(DEVICE)
optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)
loss_function = nn.CrossEntropyLoss().to(DEVICE)
# 定义学习率调度器：输入包装的模型，定义学习率衰减周期step_size，gamma为衰减的乘法因子
exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=BATCH_SIZE, gamma=0.1)
# 在官网上的解释。如果初始学习率lr = 0.05，衰减周期step_size为30，衰减乘法因子gamma=0.01
# Assuming optimizer uses lr = 0.05 for all groups
# >>> # lr = 0.05     if epoch < 30
# >>> # lr = 0.005    if 30 <= epoch < 60
# >>> # lr = 0.0005   if 60 <= epoch < 90


def train(num_epochs, _model, _device, _train_loader, _optimizer, _lr_scheduler):
    _model.train()  # 设置模型为训练模式
    _lr_scheduler.step()  # 设置学习率调度器开始准备更新
    for step, (images, labels) in enumerate(train_loader):
        # 可视化loss
        # loss_meter.reset()
        samples = images.to(_device)
        labels = labels.to(_device)
        output = cnn(samples)
        loss = loss_function(output, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        # 可视化loss
        # loss_meter.add(loss.data)
        # vis.plot_many_stack({'train_loss': float(loss_meter.value()[0])})

        if step % 100 == 0:
            print("Epoch:{}, step:{}, loss:{:.6f}".format(num_epochs, step, loss.item()))
            with open('train_record.csv', 'a') as f:
                f.write("Epoch:{}, step:{}, loss:{:.6f}\n".format(num_epochs, step, loss.item()))


def test(_test_loader, _model, _device):
    _model.eval()  # 设置模型进入预测模式 evaluation
    loss, correct = 0, 0
    # 不计算梯度，节约显存
    with torch.no_grad():
        for data, target in _test_loader:
            data, target = data.to(_device), target.to(_device)
            test_output = cnn(data)
            g = make_dot(test_output)
            g.render('cnn', view=False)
            loss += loss_function(test_output, target).item()  # 添加损失值
            pred = test_output.data.max(1, keepdim=True)[1]  # 找到概率最大的下标，为输出值
            correct += pred.eq(target.data.view_as(pred)).cpu().sum()  # .cpu()是将参数迁移到cpu上来。

        loss /= len(_test_loader.dataset)

        print('\nAverage loss: {:.6f}, Accuracy: {}/{} ({:.4f}%)\n'.format(
            loss, correct, len(_test_loader.dataset),
            100. * correct / len(_test_loader.dataset)))
        with open('train_record.csv', 'a') as f:
            f.write('\nAverage loss: {:.6f}, Accuracy: {}/{} ({:.4f}%)\n'.format(
            loss, correct, len(_test_loader.dataset),
            100. * correct / len(_test_loader.dataset)))


if __name__ == '__main__':
    for epoch in range(EPOCH):
        train(epoch, cnn, DEVICE, train_loader, optimizer, exp_lr_scheduler)
        test(test_loader, cnn, DEVICE)
    # torch.save(cnn.state_dict(), "CNNMnist.pth")